<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Joseph Zhu</title> <meta name="author" content="Joseph Zhu"> <meta name="description" content="Joseph Zhu's homepage "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/jz.jpg"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://junzhejosephzhu.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Joseph</span> Zhu </h1> <p class="desc"><a class="extra_space" href="https://www.linkedin.com/in/joseph-junzhe-zhu-38023113b/" rel="external nofollow noopener" target="_blank">Linkedin</a> <a class="extra_space" href="https://scholar.google.com/citations?user=ss3SR9YAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Google Scholar</a> <a class="extra_space" href="https://github.com/JunzheJosephZhu" rel="external nofollow noopener" target="_blank">Github</a> <a href="#" class="collapsible extra_space">Wechat</a> <a class="content">josephz2000</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/burning%20man%20profile%20picture-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/burning%20man%20profile%20picture-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/burning%20man%20profile%20picture-1400.webp"></source> <img src="/assets/img/burning%20man%20profile%20picture.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="burning man profile picture.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p>I’m a startup founder working on robotics. Over the past 3 years, I have been a cofounder and the only software engineer at Tepan, developing software for a stovetop bimanual robot that autonomously takes care of everything from ingredient prepping, cooking, seasoning, to self-cleaning.</p> <p>Prior to founding Tepan, I quit on PhD from Berkeley AI(BAIR) and briefly joined Stanford. I did some research under the supervision of Jiajun Wu in <a href="https://svl.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford Vision and Learning Lab</a>(directed by Fei-Fei Li), and with Sanmi Koyejo in <a href="https://ilwiki.stanford.edu/doku.php?id=snap-servers:snap-servers" rel="external nofollow noopener" target="_blank">SNAP</a>.</p> <h4 id="my-tech-stack-nowadays">My tech stack nowadays</h4> <p>Click each one to expand:</p> <ul> <li> <a href="#neural-networks" class="collapsible">Neural Networks</a> <span class="content" style=""><br> I get down to pretty basic levels of this. Back in undergrad when working on audio there weren’t many standardized architectures, so I usually wrote my own neural networks(e.g. convolutional networks to generate attention masks, implementing transformers using cupy, using Fourier transforms to implement convolutions, recursive RNNs, inference for WaveNets). Recently, things I’ve been doing include making a custom promptable segmentation model for food ingredients based on ViT Adapter in Detectron2, and training 3D networks that use the heat diffusion process to propagate features on meshes. <br></span> </li> <li> <a href="#generative-models" class="collapsible">Information theory/Generative Models</a> <span class="content" style=""><br> I’m pretty familiar with the theory in general, getting into the nitty gritty details of the whole derivation for diffusion models, flow matching, consistency models, and VAE, by re-deriving everything from scratch. I wrote a paper called HiFA for my side project that broke down the score distillation process of a diffusion model into a pseudo ground-truth generating process. This paper has been cited 100+ times within its first year. <br><br> I also love reading general probability theory related papers. My favorite one is GAIL, but I haven’t 100% understood the proof yet. <br><br> That said, I am ashamed to admit that “A Mathematical Theory of Communication” has been on my reading list for 5 years now and I still haven’t read it. <br></span> </li> <li> <a href="#ai-infra" class="collapsible">AI Infrastructure</a> <span class="content" style=""><br> I’m usually forced to figure this out since I’m the only software engineer at Tepan. I’ve managed all the clusters we had on GCP and spent $250k compute credits within the past year. That includes fine-tuning a DINOv2 using around 64 L4 GPUs for 10 days, while writing custom fine tuning code. Other than that, I delved deep into FSDP model sharding and figured out nitty gritty details like “why can’t BatchNorm be FSDP’ed?”, checkpointing shards vs whole models, etc, in order to train a Maskformer more efficiently with a bigger batch size. I also wrote a torchserve wrapper for this custom promptable maskformer for efficient serving. I also wrote some TensorRT back in its early days, but it didnt turn out too well because back then there was no GPT and very little support for pytorch ONNX. <br></span> </li> <li> <a href="#supply-chain" class="collapsible">Hardware Supply Chain and Data Labelling</a> <span class="content" style=""><br> Supply chain is quite important when running a hardware startup, especially if in China. That requires playing tug-of-war with suppliers, getting information and product samples, cross checking the prices, picking the right one that’s both cheap enough and good enough. I’ve built quite some familiarity with the supply chain for chips, basic brands, and a good sense of the rough range of prices for different products. <br><br> I also worked extensively with data labellers, from picking a data labelling agency to writing data manuals, and developing ways to monitor the efficiency of data labellers. I curated a dataset of 5M food images of food ingredients, with 1M of them filtered and coarsely annotated, and 50k of them annotated with segmentation masks, all with a budget of 10k USD. <br></span> </li> <li> <a href="#3d-modelling" class="collapsible">3D modelling and graphics</a> <span class="content" style=""><br> I am familiar with NeRFs and differentiable rendering pipelines, in particular NVDiffRast and Pytorch3D. I’ve invented custom losses to make NeRFs sharper by regularizing the distribution of density functions along a ray, and impelemented GGX microfacet BRDFs for NeRFs with point lights(althought that didn’t work very well because it needs an environmental map, and that was computationally infeasible). Other than that, I played quite a lot with Poisson reconstruction, and recently explored using Poisson Recon to obtain a mesh that comes with albedo, roughness, and metallic textures, before throwing that into NVDiffRast for further optimization. I have recently been learning GLFW in an attempt to render a robot along with point clouds into a VR headset with high frame rates for teleoperation.</span> </li> <li> <a href="#depth-cameras" class="collapsible">Depth Cameras, Camera Calibration, and 3D Reconstruction</a> <span class="content" style=""><br> I have done a lot of stuff for depth maps and depth cameras. I’ve worked with structured-light and active stereo cameras coming straight from suppliers like Orbbec and Deptrum, often with lacking functionalities that I have to implement myself, such as efficiently aligning depth streams and RGB streams of different resolutions using relative lens poses(they are actually misaligned since their camera holes are not the same, one is IR but the other is RGB), obtaining colored point clouds from those(surprisingly, some suppliers don’t even have that function built into their cameras SDKs…). I wrote a python binding for Deptrum SDK because they didn’t have one. <br><br> I also have a deep understanding of camera parameters and the calibration process. I had two depth cameras that I had intrinsics of but I needed to figure out their exact relative poses in order to merge their point clouds. I bought an AprilGrid tab from Taobao and realized that only Kalibr can could give me the corners accurately, but Kalibr doesnt let me directly calibrate the extrinsics given the intrinsics, so I had to handwrite Direct Linear Transform(DLT) + some ICP stuff + some RANSAC plane detection stuff to get the job done. I’m familiar with hand-eye calibration methods such as Tsai and Lenz as well. <br></span> </li> <li> <a href="#robotics-software" class="collapsible">Embedded Robotics Software</a> <span class="content" style=""><br> The entire Tepan robot was built from scratch, the maximum level of packaged-ness we got was a custom servo for some joints, but we had to take apart even that. Therefore, I wrote PID controllers for quite a few different types of motors for STM and Arduino boards(back when I was building in my garage). Theres a lot of low-level engineering involved in this, here are a few examples. Back in 2022 I modified <a href="https://www.airspayce.com/mikem/arduino/AccelStepper/" rel="external nofollow noopener" target="_blank">the arduino AccelStepper</a> library to compute stepper pulse intervals in real time based on high-frequency waypoint inputs on a mere Arduino nano(where I won’t be able to use square root/division if I don’t want to miss a pulse) by extending <a href="https://www.embedded.com/generate-stepper-motor-speed-profiles-in-real-time/" rel="external nofollow noopener" target="_blank">this paper</a>. Recently(this is probably the <b>biggest</b> engineering <b style="color: red;">NIGHTMARE</b> I’ve ever faced!), we had a custom board with which tried to run FOC on a stepper motor. There was 3 months of struggling here, where the major problems are (i) the UART communication would randomly break down, in the end there was an issue with a cyclic buffer. (ii) there is a thing called “zero electric angle” in FOC, and that was changing all the time even when it was supposed to be consistent, and it turned out that the position sensor wasn’t giving absolute but incremental angles from when it was powered on. (iii) The motor just gets stuck randomly when there was a little bit of resistance. This took the longest, initially we thought there were issues with the PWM or some backEMF stuff causing the motor to produce enough force, but in the end after countless testing it turned out that the position sensor magnet wasn’t big enough and it was causing small errors (like 0.5 degrees) in position readings, and it just so happened that stepper motor FOC was REALLY sensitive to those errors. <br></span> </li> <li> <a href="#kinematics" class="collapsible">Robotics Fundamentals</a> <span class="content" style=""><br>Tepan has a really weirdly shaped robot, with two 5 DOF arms, where one branches off from the other. I wrote custom forward kinematic with Jacobian computation using sympy, and wrote a custom null space inverse kinematic solvers(there was a teleop requirement so that I had to use euler angles to parameterize the robot, so the angular component of the Jacobian was for euler angles instead of the usual angular velocity, I had to do some tricky derivations to avoid gimbal lock). I also implemented gravity compensation from scratch. <br><br> I have designed a lot of adaptive motions for the robot. For example, there would be a pre-defined chopping motion for round things that need to be sliced, but adapted onto food items of different positions, sizes, and distortions. To this end I also did a project called <b>DenseMatcher</b>, which finds correspondences between objects in order to map one’s contact points to another. <br></span> </li> <li> <a href="#vr" class="collapsible">VR/Spatial Computing</a> <span class="content" style=""><br> I have been recently writing a program that uses two oculus VR controllers to program a robot. The key challenge is mapping the controller poses to robot poses, which was kinematically constrained, and also figuring out what’s the most intuitive way to couple the two. As an intuitive example, if the user wants to pause the robot for a bit, they wouldn’t want the robot to suddenly snap when it resumes if the controller moved in between. Another example would be, should I model the controller as rotating around an origin centered at itself, or rotating around an origin centered at some external coordinate frame, and what about the robot? If users want the robot to rotate at half of the speed of the controller, how would I model this division in rotation? A lot of fun ways to manipulate SE(3) matrices under different coordinate frames. Also some tricky transforms between different coordinate conventions(Bullet vs OpenGL, COM frame vs URDF link frame, etc). <br></span> </li> <li> <a href="#signal-processing" class="collapsible">Signal Processing</a> <span class="content" style=""><br> I worked on computational speech for quite some time and have a lot of in depth understanding of signal processing math, including convolutions Short Time Fourier Transforms, Wavelets, time varying IIR filters, as well as image signal processing stuff. Aside from published research, I worked on an AI audio codec in Tencent where I turned <a href="https://www.researchgate.net/publication/3999091_Code-excited_Linear_Prediction_CELP_High_Quality_Speech_at_Very_Low_Bit_Rates" rel="external nofollow noopener" target="_blank">Linear Predictive Coding</a>(a type of residual coding for compressing speech) into a differentiable layer in the neural network. Recently I have been working on functional maps, which is basically signal processing for meshes. It turns out that you can apply something similar to Fourier Transform to meshes, call Laplace-Beltrami eigenfunctions. <br></span> </li> <li> <a href="#physics-of-electricity" class="collapsible">Physics of Electricity &amp; Mechanics</a> <span class="content" style=""><br> I did my undergrad in EE, so I understand how analog &amp; digital circuits work on a fundamental level, and how radio/electromagnetic waves work. (Quick question: what is the difference between radiowave and laser in terms of electromagnetic field?) Thanks to the coaching of <a href="https://math.illinois.edu/resources/department-resources/syllabus-math-487" rel="external nofollow noopener" target="_blank">Prof. Jont Allen</a>, I got to learn about dynamical system, and <a href="https://auditorymodels.org/index.php?n=Courses.ECE498-ECENeuroScience-S21" rel="external nofollow noopener" target="_blank">how neurons work on the electrical level</a>. I also dabbled in fluid mechanics(Navier-Stokes for liquid, Webster Horn for gas and sound), and structural mechanics, although I’m not an expert. <br></span> </li> </ul> </div> <h2><a href="/publications/" style="color: inherit;">Publications</a></h2> I spend most of my time doing engineering, but occasionally publish research papers. <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/densematcher.png"></div> <div id="zhu2025densematcher" class="col-sm-8"> <div class="title">DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from One Demo</div> <div class="author"> <b><em>Junzhe Zhu</em></b>, Yuanchen Ju, Junyi Zhang, Muhan Wang, Zhecheng Yuan, Kaizhe Hu, and Huazhe Xu</div> <div class="periodical"> <em>In The Thirteenth International Conference on Learning Representations(ICLR)</em>, 2025 </div> <div class="periodical"> </div> <div class="award"> <span style="background-color: #e53935; color: white; font-weight: 500; padding: 3px 8px; border-radius: 4px; font-size: 0.85em;">Spotlight Award</span> </div> <div class="links"> <a href="https://tea-lab.github.io/DenseMatcher/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/abs/2412.05268" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/JunzheJosephZhu/DenseMatcher" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hifa.png"></div> <div id="zhu2024hifa" class="col-sm-8"> <div class="title">HIFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance</div> <div class="author"> <b><em>Junzhe Zhu</em></b>, Peiye Zhuang, and Sanmi Koyejo</div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations(ICLR)</em>, 2024 </div> <div class="periodical"> </div> <div class="award"> <span style="background-color: #e53935; color: white; font-weight: 500; padding: 3px 8px; border-radius: 4px; font-size: 0.85em;"><a href="https://huggingface.co/papers/2305.18766" style="color: white;" rel="external nofollow noopener" target="_blank">HuggingFace Daily Paper Top 2</a></span> </div> <div class="links"> <a href="https://josephzhu.com/HiFA-site/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project Page</a> <a href="https://arxiv.org/abs/2305.18766" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/JunzheJosephZhu/HiFA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/seehearfeel.png"></div> <div id="li2022seehearfeel" class="col-sm-8"> <div class="title">See, Hear, and Feel: Smart Sensory Fusion for Robotic Manipulation</div> <div class="author"> Hao Li*, Yizhi Zhang*, <b><em>Junzhe Zhu</em></b>, Shaoxiong Wang, Michelle A. Lee, Huazhe Xu, Edward Adelson, Li Fei-Fei, Ruohan Gao, and Jiajun Wu</div> <div class="periodical"> <em>In The Conference on Robot Learning (CoRL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/seehearfeel.html" class="btn btn-sm z-depth-0" role="button">Project Page</a> <a href="https://arxiv.org/abs/2212.03858" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/JunzheJosephZhu/see_hear_feel" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/multidecoder.png"></div> <div id="zhu2021multidecoderdprnn" class="col-sm-8"> <div class="title">Multi-Decoder DPRNN: Source Separation for Variable Number of Speakers</div> <div class="author"> <b><em>Junzhe Zhu</em></b>, Raymond A. Yeh, and Mark Hasegawa-Johnson</div> <div class="periodical"> <em>In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://junzhejosephzhu.github.io/Multi-Decoder-DPRNN/" class="btn btn-sm z-depth-0" role="button">Project Page</a> <a href="http://www.isle.illinois.edu/speech_web_lg/pubs/2021/zhu2021multi.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/asteroid-team/asteroid/tree/master/egs/wsj0-mix-var/Multi-Decoder-DPRNN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/littlebeats.png"></div> <div id="zhu2021littlebeats" class="col-sm-8"> <div class="title">A Comparison Study on Infant-Parent Voice Diarization</div> <div class="author"> <b><em>Junzhe Zhu</em></b>, Mark Hasegawa-Johnson, and Nancy McElwain</div> <div class="periodical"> <em>In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="http://www.isle.illinois.edu/speech_web_lg/pubs/2021/zhu2021comparison.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/JunzheJosephZhu/Child_Speech_Diarization" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/identifycocktail.png"></div> <div id="zhu2020identifyspeakers" class="col-sm-8"> <div class="title">Identify Speakers in Cocktail Parties with End-to-End Attention</div> <div class="author"> <b><em>Junzhe Zhu</em></b>, Mark Hasegawa-Johnson, and Leda Sari</div> <div class="periodical"> <em>In Interspeech 2020: 21st Annual Conference of the International Speech Communication Association</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="http://www.interspeech2020.org/uploadfile/pdf/Wed-3-2-6.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/JunzheJosephZhu/Identify-Speakers-in-Cocktail-Parties-with-E2E-Attention" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/topicmodel.png"></div> <div id="10.1145/3453460.3453462" class="col-sm-8"> <div class="title">A Machine Learning Algorithm for Sorting Online Comments via Topic Modeling</div> <div class="author"> <b><em>Junzhe Zhu</em></b>, Elizabeth Wickes, and John R. Gallagher</div> <div class="periodical"> <em>Commun. Des. Q. Rev</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://sigdoc.acm.org/wp-content/uploads/2021/06/CDQ_9.2_2021.pdf#page=4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This article uses a machine learning algorithm to demonstrate a proof-of-concept case for moderating and managing online comments as a form of content moderation, which is an emerging area of interest for technical and professional communication (TPC) researchers. The algorithm sorts comments by topical similarity to a reference comment/article rather than display comments by linear time and popularity. This approach has the practical benefit of enabling TPC researchers to reconceptualize content display systems in dynamic ways.</p> </div> </div> </div> </li> </ol> </div> <h2><a style="color: inherit;">Places I worked at</a></h2> <div class="work"> <ol class="experiences"> <li> <div class="row hoverable rounded collapsible"> <div class="col-sm-3"><img class="logo" src="/assets/img/Facebook-logo.png"></div> <div class="col-sm-3"><h2>Facebook</h2></div> <div class="col-sm-1 align-right"><img class="icon" src="/assets/img/expand_button.png"></div> </div> <div class="content2" display="none"> I was working at Reality Lab's AI team. My job was to write <b>a distillation frameworks for activity detection models</b> running on AR glasses. Basically, the concept is that inside Facebook, we have the data &amp; compute to train huge self-supervised "foundation models"(like VLP transformer, etc) using video-language-audio contrastive training and such such, but those models are too big to be deployed on the AR glasses. On the other hand, the "on-glass" model are small video processing neural networks(like MViT, X3D, etc), but those have lower accuracy if trained normally with supervised learning. My job was to write a framework where we can extract features &amp; soft probability predictions from the big models, and train the smaller models using those. This is a huge performance booster, I was able to boost the mAP on a 100+ category video classification model from 0.32 to 0.49. </div> </li> <li> <div class="row hoverable rounded collapsible"> <div class="col-sm-3"><img class="logo" src="/assets/img/tencent-logo.png"></div> <div class="col-sm-3"><h2>Tencent</h2></div> <div class="col-sm-1 align-right"><img class="icon" src="/assets/img/expand_button.png"></div> </div> <div class="content2" display="none"> Encoding human speech at 2kb/s bitrate to let you have meetings in an underground parking structure/elevator. Mingled a bunch of GAN(Generative Adversarial Networks) and signal compression stuff made in Bell Labs from the 80s. I forgot to save a recording when I left the job, this is something I managed to produce halfway through the internship.<br> <audio controls src="/assets/audio/CN_F1_gan.wav"></audio> </div> </li> <li> <div class="row hoverable rounded collapsible"> <div class="col-sm-3"><img class="logo" src="/assets/img/MIT-logo.png"></div> <div class="col-sm-3"><h2>MIT</h2></div> <div class="col-sm-1 align-right"><img class="icon" src="/assets/img/expand_button.png"></div> </div> <div class="content2" display="none"> In 2019 when I was a sophomore, I was really intrigued by neural networks(not the ones you train, I'm talking about the ones in the human brain) and the PDE(partial differential equation) that governs it, so I read <a href="http://jontalle.web.engr.illinois.edu/Public/Scott-Neuroscience.02.pdf" rel="external nofollow noopener" target="_blank">this book</a>. I wanted to do research on it, so I was planning on going to go to MIT's <a href="https://mcgovern.mit.edu/" rel="external nofollow noopener" target="_blank">McGovern Institue</a> to do some research in Summer 2020. However, Covid hit, and MIT completely stopped hiring staff, so I couldn't go there in person. I winded up doing some part-time remote research. This was the <a href="https://github.com/JunzheJosephZhu/metalearning_pytorch.git" rel="external nofollow noopener" target="_blank">code</a>. </div> </li> <li> <div class="row hoverable rounded collapsible"> <div class="col-sm-3"><img class="logo" style="margin-top: 30px; margin-bottom: 30px;" src="/assets/img/sensetime-logo.png"></div> <div class="col-sm-3"><h2>Sensetime</h2></div> <div class="col-sm-1 align-right"><img class="icon" src="/assets/img/expand_button.png"></div> </div> <div class="content2" display="none"> <iframe width="560" height="315" src="https://www.youtube.com/embed/HKiaQ_Qgk9s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> </div> </li> <li> <div class="row hoverable rounded collapsible"> <div class="col-sm-3"><img class="logo" src="/assets/img/capital-one.png"></div> <div class="col-sm-3"><h2>Capital One</h2></div> <div class="col-sm-1 align-right"><img class="icon" src="/assets/img/expand_button.png"></div> </div> <div class="content2" display="none">Did some NLP stuff, like sentiment analysis, in the anti-money-laundering department</div> </li> <li> <div class="row hoverable rounded collapsible"> <div class="col-sm-3"><img class="logo" style="margin-top: 20px; margin-bottom: 20px;" src="/assets/img/ijet.jpg"></div> <div class="col-sm-3"><h2>i-jet Lab</h2></div> <div class="col-sm-1 align-right"><img class="icon" src="/assets/img/expand_button.png"></div> </div> <div class="content2" display="none"> I was learning things like object detection, semantic segmentation, path planning. I got the chance to work on a lot of cool projects. For one here, I am teaching a boat how to dock itself:<br> <iframe width="560" height="315" src="https://www.youtube.com/embed/o-254pc2SDU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><br> I also made <a href="https://youtube.com/shorts/082UIG4ZXog?feature=share" rel="external nofollow noopener" target="_blank">an imitation learning proof-of-concept with an omnidirectional robot for docking.</a><br> I took care of a variety of other projects, like doing NLP stuff on 600k patents, write the trajectory for a drone to take videos, classifying satillite images to analyze our users, etc. </div> </li> </ol> </div> <h2><a href="/previous/" style="color: inherit;">Previous Life</a></h2> <div class="previous"> <p>I got my Bachelor’s Degree in Electrical Engineering at <a href="https://en.wikipedia.org/wiki/University_of_Illinois_Urbana-Champaign" rel="external nofollow noopener" target="_blank">University of Illiois at Urbana Champaign</a>, where I was advised by <a href="http://www.ifp.illinois.edu/~hasegawa/" rel="external nofollow noopener" target="_blank">Prof. Mark Hasegawa-Johnson</a> and <a href="https://www.youtube.com/watch?v=7u7vIZMoXXo" rel="external nofollow noopener" target="_blank">Prof. Jont Allen</a>, specializing in speech signal processing &amp; machine learning. I did a college speedrun by graduating under 2.5 years, with the end mostly being research.</p> <h4 id="how-i-started-doing-engineering">How I started doing engineering</h4> <p>In high school I was specializing in literature(like Shakespearean early-modern English or early 1800s American poetry). At the beginning of college, I saw GPT-2 could write essays, so I started learning python and training an LSTM on Shakespearean poetry. Later on I also played with computer vision and audio. I did an internship at Brunswick Corporation working on self-driving boats using imitation learning, and then did some research and published 4 papers about computational audio and language.</p> <h4 id="my-pre-meditated-dropout-from-stanford">My pre-meditated dropout from Stanford</h4> <p>When I finished undergrad, I was accepted to the EECS PhD program at <a href="https://bair.berkeley.edu/" rel="external nofollow noopener" target="_blank">Berkely AI Reserach</a> with full scholarship and some additional stipends, as well as CS program from Stanford. I knew I wasn’t gonna do research for another 5 years and would probably drop out soon to do a startup, so I enrolled in the latter and dropped out around a year later.</p> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%75%6E%7A%68%65.%6A%6F%73%65%70%68.%7A%68%75@%67%6D%61%69%6C.%63%6F%6D" target="_blank" title="email"><i class="fas fa-envelope"></i></a> <a href="assets/img/wechat.jpg" target="_blank" title="Wechat id"><i class="fab fa-weixin" aria-hidden="true"></i></a> </div> <div class="contact-note"> Feel free to add me on Wechat or send an email if you want to get in touch </div> </div> </article> </div> <script src="/assets/js/collapse.js"></script> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Joseph Zhu. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>